import pytest


@pytest.fixture()
def generate_output() -> dict:
    return {"results": [{"text": "output text"}]}


@pytest.fixture()
def model_not_loaded_output() -> dict:
    return {
        "result": {
            "model_name": "None",
            "lora_names": [],
            "shared.settings": {
                "dark_theme": True,
                "show_controls": True,
                "start_with": "",
                "mode": "chat",
                "chat_style": "TheEncrypted777",
                "character": "None",
                "prompt-default": "QA",
                "prompt-notebook": "QA",
                "preset": "simple-1",
                "max_new_tokens": 200,
                "max_new_tokens_min": 1,
                "max_new_tokens_max": 4096,
                "seed": -1,
                "negative_prompt": "",
                "truncation_length": 2048,
                "truncation_length_min": 0,
                "truncation_length_max": 16384,
                "custom_stopping_strings": "",
                "auto_max_new_tokens": False,
                "max_tokens_second": 0,
                "ban_eos_token": False,
                "add_bos_token": True,
                "skip_special_tokens": True,
                "stream": True,
                "name1": "You",
                "name2": "Assistant",
                "context": (
                    "This is a conversation with your Assistant. It is a computer program designed to help you"
                    " with various tasks such as answering questions, providing recommendations, and helping with decision "
                    "making. You can ask it anything you want and it will do its best "
                    "to give you accurate and relevant information."
                ),
                "greeting": "",
                "instruction_template": "Alpaca",
                "chat-instruct_command": (
                    "Continue the chat dialogue below. Write a single reply "
                    ' for the character "<|character|>".\n\n<|prompt|>'
                ),
                "autoload_model": False,
                "default_extensions": ["gallery"],
            },
            "shared.args": {
                "notebook": False,
                "chat": False,
                "multi_user": False,
                "character": None,
                "model": None,
                "lora": None,
                "model_dir": "models/",
                "lora_dir": "loras/",
                "model_menu": False,
                "no_stream": False,
                "settings": None,
                "extensions": ["api", "gallery"],
                "verbose": False,
                "loader": None,
                "cpu": False,
                "auto_devices": False,
                "gpu_memory": None,
                "cpu_memory": None,
                "disk": False,
                "disk_cache_dir": "cache",
                "load_in_8bit": False,
                "bf16": False,
                "no_cache": False,
                "xformers": False,
                "sdp_attention": False,
                "trust_remote_code": False,
                "load_in_4bit": False,
                "compute_dtype": "float16",
                "quant_type": "nf4",
                "use_double_quant": False,
                "threads": 0,
                "n_batch": 512,
                "no_mmap": False,
                "low_vram": False,
                "mlock": False,
                "mul_mat_q": False,
                "cache_capacity": None,
                "n_gpu_layers": 0,
                "tensor_split": None,
                "n_ctx": 2048,
                "llama_cpp_seed": 0,
                "n_gqa": 0,
                "rms_norm_eps": 0,
                "wbits": 0,
                "model_type": None,
                "groupsize": -1,
                "pre_layer": None,
                "checkpoint": None,
                "monkey_patch": False,
                "triton": False,
                "no_inject_fused_attention": False,
                "no_inject_fused_mlp": False,
                "no_use_cuda_fp16": False,
                "desc_act": False,
                "disable_exllama": False,
                "gpu_split": None,
                "max_seq_len": 2048,
                "cfg_cache": False,
                "deepspeed": False,
                "nvme_offload_dir": None,
                "local_rank": 0,
                "rwkv_strategy": None,
                "rwkv_cuda_on": False,
                "alpha_value": 1,
                "rope_freq_base": 0,
                "compress_pos_emb": 1,
                "listen": False,
                "listen_host": None,
                "listen_port": None,
                "share": False,
                "auto_launch": False,
                "gradio_auth": None,
                "gradio_auth_path": None,
                "ssl_keyfile": None,
                "ssl_certfile": None,
                "api": True,
                "api_blocking_port": 5000,
                "api_streaming_port": 5005,
                "public_api": False,
                "public_api_id": None,
                "multimodal_pipeline": None,
            },
        }
    }


@pytest.fixture()
def model_loaded_output() -> dict:
    return {
        "result": {
            "model_name": "codellama-7b-instruct.Q4_K_M.gguf",
            "lora_names": ["todo-actual-value"],
            "shared.settings": {
                "dark_theme": True,
                "show_controls": True,
                "start_with": "",
                "mode": "chat",
                "chat_style": "TheEncrypted777",
                "character": "None",
                "prompt-default": "QA",
                "prompt-notebook": "QA",
                "preset": "simple-1",
                "max_new_tokens": 200,
                "max_new_tokens_min": 1,
                "max_new_tokens_max": 4096,
                "seed": -1,
                "negative_prompt": "",
                "truncation_length": 2048,
                "truncation_length_min": 0,
                "truncation_length_max": 16384,
                "custom_stopping_strings": "",
                "auto_max_new_tokens": False,
                "max_tokens_second": 0,
                "ban_eos_token": False,
                "add_bos_token": True,
                "skip_special_tokens": True,
                "stream": True,
                "name1": "You",
                "name2": "Assistant",
                "context": (
                    "This is a conversation with your Assistant. It is a computer "
                    "program designed to help you with various tasks such as answering "
                    "questions, providing recommendations, and helping with decision "
                    "making. You can ask it anything you want and it will do its best to "
                    "give you accurate and relevant information."
                ),
                "greeting": "",
                "instruction_template": "Llama-v2",
                "chat-instruct_command": (
                    "Continue the chat dialogue below. Write a single reply for the "
                    'character "".\n\n'
                ),
                "autoload_model": False,
                "default_extensions": ["gallery"],
                "wbits": "None",
                "model_type": "llama",
                "groupsize": "None",
                "pre_layer": 0,
                "n_gqa": 0,
                "rms_norm_eps": 0,
                "rope_freq_base": 1000000,
            },
            "shared.args": {
                "notebook": False,
                "chat": False,
                "multi_user": False,
                "character": None,
                "model": "codellama-7b-instruct.Q4_K_M.gguf",
                "lora": None,
                "model_dir": "models/",
                "lora_dir": "loras/",
                "model_menu": False,
                "no_stream": False,
                "settings": None,
                "extensions": ["api", "gallery"],
                "verbose": False,
                "loader": "ctransformers",
                "cpu": False,
                "auto_devices": False,
                "gpu_memory": ["8"],
                "cpu_memory": "1",
                "disk": False,
                "disk_cache_dir": "cache",
                "load_in_8bit": False,
                "bf16": False,
                "no_cache": False,
                "xformers": False,
                "sdp_attention": False,
                "trust_remote_code": False,
                "load_in_4bit": False,
                "compute_dtype": "float16",
                "quant_type": "nf4",
                "use_double_quant": False,
                "threads": 0,
                "n_batch": 512,
                "no_mmap": False,
                "low_vram": False,
                "mlock": False,
                "mul_mat_q": False,
                "cache_capacity": None,
                "n_gpu_layers": 1000000,
                "tensor_split": None,
                "n_ctx": 2500,
                "llama_cpp_seed": 0,
                "n_gqa": 0,
                "rms_norm_eps": 0,
                "wbits": 0,
                "model_type": "llama",
                "groupsize": -1,
                "pre_layer": None,
                "checkpoint": None,
                "monkey_patch": False,
                "triton": False,
                "no_inject_fused_attention": False,
                "no_inject_fused_mlp": False,
                "no_use_cuda_fp16": False,
                "desc_act": False,
                "disable_exllama": False,
                "gpu_split": None,
                "max_seq_len": 2048,
                "cfg_cache": False,
                "deepspeed": False,
                "nvme_offload_dir": None,
                "local_rank": 0,
                "rwkv_strategy": None,
                "rwkv_cuda_on": False,
                "alpha_value": 1,
                "rope_freq_base": 1000000,
                "compress_pos_emb": 1,
                "listen": True,
                "listen_host": None,
                "listen_port": None,
                "share": False,
                "auto_launch": False,
                "gradio_auth": None,
                "gradio_auth_path": None,
                "ssl_keyfile": None,
                "ssl_certfile": None,
                "api": True,
                "api_blocking_port": 5000,
                "api_streaming_port": 5005,
                "public_api": False,
                "public_api_id": None,
                "multimodal_pipeline": None,
            },
        }
    }


@pytest.fixture()
def load_model_output() -> dict:
    return {
        "result": {
            "model_name": "codellama-7b-instruct.Q4_K_M.gguf",
            "lora_names": [],
            "shared.settings": {
                "dark_theme": True,
                "show_controls": True,
                "start_with": "",
                "mode": "chat",
                "chat_style": "TheEncrypted777",
                "character": "None",
                "prompt-default": "QA",
                "prompt-notebook": "QA",
                "preset": "simple-1",
                "max_new_tokens": 200,
                "max_new_tokens_min": 1,
                "max_new_tokens_max": 4096,
                "seed": -1,
                "negative_prompt": "",
                "truncation_length": 2048,
                "truncation_length_min": 0,
                "truncation_length_max": 16384,
                "custom_stopping_strings": "",
                "auto_max_new_tokens": False,
                "max_tokens_second": 0,
                "ban_eos_token": False,
                "add_bos_token": True,
                "skip_special_tokens": True,
                "stream": True,
                "name1": "You",
                "name2": "Assistant",
                "context": (
                    "This is a conversation with your Assistant. It is a computer "
                    "program designed to help you with various tasks such as answering "
                    "questions, providing recommendations, and helping with decision "
                    "making. You can ask it anything you want and it will do its best to "
                    "give you accurate and relevant information."
                ),
                "greeting": "",
                "instruction_template": "Llama-v2",
                "chat-instruct_command": (
                    "Continue the chat dialogue below. Write a single reply for the "
                    'character "".\n\n'
                ),
                "autoload_model": False,
                "default_extensions": ["gallery"],
                "wbits": "None",
                "model_type": "llama",
                "groupsize": "None",
                "pre_layer": 0,
                "n_gqa": 0,
                "rms_norm_eps": 0,
                "rope_freq_base": 1000000,
            },
            "shared.args": {
                "notebook": False,
                "chat": False,
                "multi_user": False,
                "character": None,
                "model": "codellama-7b-instruct.Q4_K_M.gguf",
                "lora": None,
                "model_dir": "models/",
                "lora_dir": "loras/",
                "model_menu": False,
                "no_stream": False,
                "settings": None,
                "extensions": ["api", "gallery"],
                "verbose": False,
                "loader": "ctransformers",
                "cpu": False,
                "auto_devices": False,
                "gpu_memory": ["8"],
                "cpu_memory": "1",
                "disk": False,
                "disk_cache_dir": "cache",
                "load_in_8bit": False,
                "bf16": False,
                "no_cache": False,
                "xformers": False,
                "sdp_attention": False,
                "trust_remote_code": False,
                "load_in_4bit": False,
                "compute_dtype": "float16",
                "quant_type": "nf4",
                "use_double_quant": False,
                "threads": 0,
                "n_batch": 512,
                "no_mmap": False,
                "low_vram": False,
                "mlock": False,
                "mul_mat_q": False,
                "cache_capacity": None,
                "n_gpu_layers": 1000000,
                "tensor_split": None,
                "n_ctx": 2500,
                "llama_cpp_seed": 0,
                "n_gqa": 0,
                "rms_norm_eps": 0,
                "wbits": 0,
                "model_type": "llama",
                "groupsize": -1,
                "pre_layer": None,
                "checkpoint": None,
                "monkey_patch": False,
                "triton": False,
                "no_inject_fused_attention": False,
                "no_inject_fused_mlp": False,
                "no_use_cuda_fp16": False,
                "desc_act": False,
                "disable_exllama": False,
                "gpu_split": None,
                "max_seq_len": 2048,
                "cfg_cache": False,
                "deepspeed": False,
                "nvme_offload_dir": None,
                "local_rank": 0,
                "rwkv_strategy": None,
                "rwkv_cuda_on": False,
                "alpha_value": 1,
                "rope_freq_base": 1000000,
                "compress_pos_emb": 1,
                "listen": True,
                "listen_host": None,
                "listen_port": None,
                "share": False,
                "auto_launch": False,
                "gradio_auth": None,
                "gradio_auth_path": None,
                "ssl_keyfile": None,
                "ssl_certfile": None,
                "api": True,
                "api_blocking_port": 5000,
                "api_streaming_port": 5005,
                "public_api": False,
                "public_api_id": None,
                "multimodal_pipeline": None,
                "n-gpu-layers": 100,
            },
        }
    }
